{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEMO 2: **Byte-Pair Encoding Tokenization**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo run of Byte-Pair Encoding Tokenization training process over the TinyStories dataset. This run uses a Macbook Pro 2023, M3 Pro. \n",
    "\n",
    "The implementation uses a greedy approach for the inference. The greediness provides more efficiency in runtime for the tokenization process; Some studies, such as [Greed is All You Need: An Evaluation of Tokenizer Inference Methods](https://arxiv.org/pdf/2403.01289) (2024), show that greedy inference also yields good results in benchmarks, especially for morphologically-motivated tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPETokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's instantiate a BPE Tokenizer with the `BPETokenizer` class. The only special token we'll be considering is the ending token of the TinyStories dataset, which is `\"<|endoftext|>\"`. Let's try a vocab size of `10000`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bpe_transformer.tokenization import BPETrainer\n",
    "\n",
    "special_tokens = [\"<|endoftext|>\"]\n",
    "bpe = BPETrainer(vocab_size=10000, special_tokens=special_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before the training process, the pre-tokenization functionalities are called to pre-process the training data, following the patterns used for GPT-2. We always keep the special tokens intact, and they are never used in the training process.\n",
    "\n",
    "For more details on it, check `1_pretokenization.ipynb`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining input variables:** Training data file path and number of workers for parallel pre-tokenization. For this notebook, we'll use the max. number of CPUs available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "input_path = Path(\"../data/TinyStoriesV2-GPT4-train.txt\")\n",
    "n_cpus = cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can train our `bpe` calling `bpe.train(..)` and track the total time taken. For memory tracking, we use `tracemalloc`.\n",
    "\n",
    "We'll also serialize the resulting vocabulary and merges to disk for further inspection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tracemalloc\n",
    "\n",
    "from time import time\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tracemalloc.start()\n",
    "    start = time()\n",
    "    bpe.train(input_path=input_path, num_processes=n_cpus)\n",
    "    end = time()\n",
    "    total_time = end - start\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, we can access our `vocab` and check some of the tokens with `bpe.vocab`, such as the longest token, as well as the merges executed (`bpe.merges`). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Peak memory usage: 0.05 GB\n",
      "--------------------------------------------------\n",
      "TRAINING TIME (seconds): 1645.2148880958557\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "BPE Tokenizer vocab (post-training, last 30 tokens)\n",
      "==================================================\n",
      "[(9970, b' Din'), (9971, b'dest'), (9972, b'Maddy'), (9973, b'Everything'), (9974, b'Curious'), (9975, b' racers'), (9976, b' patients'), (9977, b' muster'), (9978, b' god'), (9979, b' deserves'), (9980, b' aloud'), (9981, b' Things'), (9982, b'aps'), (9983, b'Use'), (9984, b' Squee'), (9985, b' Dragon'), (9986, b' tours'), (9987, b' meets'), (9988, b' marvel'), (9989, b' Rusty'), (9990, b' Liza'), (9991, b' Jet'), (9992, b'Froggy'), (9993, b' wrapper'), (9994, b' Reddy'), (9995, b' Hops'), (9996, b' Crusty'), (9997, b' whiskers'), (9998, b' nicest'), (9999, b' improving')]\n",
      "\n",
      "\n",
      "==================================================\n",
      "BPE Tokenizer merges list (post-training, last 30 merges)\n",
      "==================================================\n",
      "[(b' ', b't'), (b'h', b'e'), (b' ', b'a'), (b' ', b's'), (b' ', b'w'), (b'n', b'd'), (b' t', b'he'), (b'e', b'd'), (b' ', b'b'), (b' t', b'o'), (b' a', b'nd'), (b' ', b'h'), (b' ', b'f'), (b'i', b'n'), (b' ', b'T'), (b' w', b'a'), (b'r', b'e'), (b'i', b't'), (b'o', b'u'), (b' ', b'l'), (b' ', b'd'), (b' ', b'c'), (b' ', b'p'), (b'a', b'y'), (b' ', b'm'), (b'e', b'r'), (b' wa', b's'), (b'o', b'm'), (b' T', b'he'), (b' ', b'he')]\n"
     ]
    }
   ],
   "source": [
    "l_50 = 50 * \"=\"\n",
    "l_50_2 = 50 * \"-\"\n",
    "print(l_50)\n",
    "print(f\"Peak memory usage: {peak / 1024**3:.2f} GB\")\n",
    "print(l_50_2)\n",
    "print(f\"TRAINING TIME (seconds): {total_time}\")\n",
    "print(l_50)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "print(l_50)\n",
    "print(\"BPE Tokenizer vocab (post-training, last 30 tokens)\")\n",
    "print(l_50)\n",
    "\n",
    "print(list(bpe.vocab.items())[-30:])\n",
    "\n",
    "print(\"\\n\")\n",
    "print(l_50)\n",
    "print(\"BPE Tokenizer merges list (post-training, last 30 merges)\")\n",
    "print(l_50)\n",
    "print(bpe.merges[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the longest token in our vocab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Longest token ID: 7164\n",
      "Longest token: b' accomplishment'\n",
      "Length: 15 bytes\n",
      "Decoded:  accomplishment\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(2 * l_50)\n",
    "longest_token = max(bpe.vocab.items(), key=lambda x: len(x[1]))\n",
    "print(f\"Longest token ID: {longest_token[0]}\")\n",
    "print(f\"Longest token: {longest_token[1]}\")\n",
    "print(f\"Length: {len(longest_token[1])} bytes\")\n",
    "print(f\"Decoded: {longest_token[1].decode('utf-8', errors='replace')}\")\n",
    "print(2 * l_50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The longest token is ` accomplishment`, a complete word preceded by a whitespace.\n",
    "\n",
    "Let's export our vocab (we'll do `.txt` files in this notebook for easy and casual inspection):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ./bpe_vocab.txt and ./bpe_merges.txt\n"
     ]
    }
   ],
   "source": [
    "from os import makedirs\n",
    "\n",
    "output_dir = \".\"\n",
    "makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Export vocabulary\n",
    "with open(f\"{output_dir}/bpe_vocab.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for token, idx in bpe.vocab.items():\n",
    "        f.write(f\"{idx}\\t{token}\\n\")\n",
    "\n",
    "# Export merges\n",
    "with open(f\"{output_dir}/bpe_merges.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for pair, merged in bpe.merges:\n",
    "        f.write(f\"{pair} -> {merged} \\n\")\n",
    "\n",
    "print(f\"Saved to {output_dir}/bpe_vocab.txt and {output_dir}/bpe_merges.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
