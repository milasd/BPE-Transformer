{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEMO 2: **Byte-Pair Encoding Tokenization**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo run of Byte-Pair Encoding Tokenization training process over the TinyStories dataset. This run uses a Macbook Pro 2023, M3 Pro. \n",
    "\n",
    "The implementation uses a greedy approach for the inference. The greediness provides more efficiency in runtime for the tokenization process; Some studies, such as [Greed is All You Need: An Evaluation of Tokenizer Inference Methods](https://arxiv.org/pdf/2403.01289) (2024), show that greedy inference also yields good results in benchmarks, especially for morphologically-motivated tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPETokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's instantiate a BPE Tokenizer with the `BPETokenizer` class. The only special token we'll be considering is the ending token of the TinyStories dataset, which is `\"<|endoftext|>\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bpe_transformer.tokenization.bpe_tokenizer import BPETokenizer\n",
    "\n",
    "special_tokens = [\"<|endoftext|>\"]\n",
    "bpe = BPETokenizer(vocab_size=300000, special_tokens=special_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the training process, the pre-tokenization function is used, following the patterns used for GPT-2. For more details on it, check `1_pretokenization.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining input variables:** Training data file path and number of workers for parallel pre-tokenization. For this notebook, we'll use the max. number of CPUs available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "input_path = Path(\"../data/TinyStoriesV2-GPT4-train.txt\")\n",
    "n_cpus = cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can train our `bpe` calling `bpe.train(..)` and track the total time taken:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start = time()\n",
    "    bpe.train(input_path=input_path, num_processes=n_cpus)\n",
    "    end = time()\n",
    "    total_time = start - end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's visualize part of our resulting vocabulary and the list of merges. \n",
    "\n",
    "We'll also look at the merge cache data, which is a heap that is used during the training to keep track of the pairs and the number of occurences. \n",
    "\n",
    "You can notice that the first element, which is the frequency/count of the associated token, is negative; We use a heap to keep track of the count of each token, and since python's `heapq` implementation is a **min. heap**, we always multiply the number of occurences by (-1) as to keep the merges ordered from most frequent to least frequent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "TRAINING TIME (seconds):\n",
      "==================================================\n",
      "41.91362190246582\n",
      "\n",
      "\n",
      "==================================================\n",
      "BPE Tokenizer vocab (post-training)\n",
      "==================================================\n",
      "{0: b'\\x00', 1: b'\\x01', 2: b'\\x02', 3: b'\\x03', 4: b'\\x04', 5: b'\\x05', 6: b'\\x06', 7: b'\\x07', 8: b'\\x08', 9: b'\\t', 10: b'\\n', 11: b'\\x0b', 12: b'\\x0c', 13: b'\\r', 14: b'\\x0e', 15: b'\\x0f', 16: b'\\x10', 17: b'\\x11', 18: b'\\x12', 19: b'\\x13', 20: b'\\x14', 21: b'\\x15', 22: b'\\x16', 23: b'\\x17', 24: b'\\x18', 25: b'\\x19', 26: b'\\x1a', 27: b'\\x1b', 28: b'\\x1c', 29: b'\\x1d', 30: b'\\x1e', 31: b'\\x1f', 32: b' ', 33: b'!', 34: b'\"', 35: b'#', 36: b'$', 37: b'%', 38: b'&', 39: b\"'\", 40: b'(', 41: b')', 42: b'*', 43: b'+', 44: b',', 45: b'-', 46: b'.', 47: b'/', 48: b'0', 49: b'1', 50: b'2', 51: b'3', 52: b'4', 53: b'5', 54: b'6', 55: b'7', 56: b'8', 57: b'9', 58: b':', 59: b';', 60: b'<', 61: b'=', 62: b'>', 63: b'?', 64: b'@', 65: b'A', 66: b'B', 67: b'C', 68: b'D', 69: b'E', 70: b'F', 71: b'G', 72: b'H', 73: b'I', 74: b'J', 75: b'K', 76: b'L', 77: b'M', 78: b'N', 79: b'O', 80: b'P', 81: b'Q', 82: b'R', 83: b'S', 84: b'T', 85: b'U', 86: b'V', 87: b'W', 88: b'X', 89: b'Y', 90: b'Z', 91: b'[', 92: b'\\\\', 93: b']', 94: b'^', 95: b'_', 96: b'`', 97: b'a', 98: b'b', 99: b'c', 100: b'd', 101: b'e', 102: b'f', 103: b'g', 104: b'h', 105: b'i', 106: b'j', 107: b'k', 108: b'l', 109: b'm', 110: b'n', 111: b'o', 112: b'p', 113: b'q', 114: b'r', 115: b's', 116: b't', 117: b'u', 118: b'v', 119: b'w', 120: b'x', 121: b'y', 122: b'z', 123: b'{', 124: b'|', 125: b'}', 126: b'~', 127: b'\\x7f', 128: b'\\x80', 129: b'\\x81', 130: b'\\x82', 131: b'\\x83', 132: b'\\x84', 133: b'\\x85', 134: b'\\x86', 135: b'\\x87', 136: b'\\x88', 137: b'\\x89', 138: b'\\x8a', 139: b'\\x8b', 140: b'\\x8c', 141: b'\\x8d', 142: b'\\x8e', 143: b'\\x8f', 144: b'\\x90', 145: b'\\x91', 146: b'\\x92', 147: b'\\x93', 148: b'\\x94', 149: b'\\x95', 150: b'\\x96', 151: b'\\x97', 152: b'\\x98', 153: b'\\x99', 154: b'\\x9a', 155: b'\\x9b', 156: b'\\x9c', 157: b'\\x9d', 158: b'\\x9e', 159: b'\\x9f', 160: b'\\xa0', 161: b'\\xa1', 162: b'\\xa2', 163: b'\\xa3', 164: b'\\xa4', 165: b'\\xa5', 166: b'\\xa6', 167: b'\\xa7', 168: b'\\xa8', 169: b'\\xa9', 170: b'\\xaa', 171: b'\\xab', 172: b'\\xac', 173: b'\\xad', 174: b'\\xae', 175: b'\\xaf', 176: b'\\xb0', 177: b'\\xb1', 178: b'\\xb2', 179: b'\\xb3', 180: b'\\xb4', 181: b'\\xb5', 182: b'\\xb6', 183: b'\\xb7', 184: b'\\xb8', 185: b'\\xb9', 186: b'\\xba', 187: b'\\xbb', 188: b'\\xbc', 189: b'\\xbd', 190: b'\\xbe', 191: b'\\xbf', 192: b'\\xc0', 193: b'\\xc1', 194: b'\\xc2', 195: b'\\xc3', 196: b'\\xc4', 197: b'\\xc5', 198: b'\\xc6', 199: b'\\xc7', 200: b'\\xc8', 201: b'\\xc9', 202: b'\\xca', 203: b'\\xcb', 204: b'\\xcc', 205: b'\\xcd', 206: b'\\xce', 207: b'\\xcf', 208: b'\\xd0', 209: b'\\xd1', 210: b'\\xd2', 211: b'\\xd3', 212: b'\\xd4', 213: b'\\xd5', 214: b'\\xd6', 215: b'\\xd7', 216: b'\\xd8', 217: b'\\xd9', 218: b'\\xda', 219: b'\\xdb', 220: b'\\xdc', 221: b'\\xdd', 222: b'\\xde', 223: b'\\xdf', 224: b'\\xe0', 225: b'\\xe1', 226: b'\\xe2', 227: b'\\xe3', 228: b'\\xe4', 229: b'\\xe5', 230: b'\\xe6', 231: b'\\xe7', 232: b'\\xe8', 233: b'\\xe9', 234: b'\\xea', 235: b'\\xeb', 236: b'\\xec', 237: b'\\xed', 238: b'\\xee', 239: b'\\xef', 240: b'\\xf0', 241: b'\\xf1', 242: b'\\xf2', 243: b'\\xf3', 244: b'\\xf4', 245: b'\\xf5', 246: b'\\xf6', 247: b'\\xf7', 248: b'\\xf8', 249: b'\\xf9', 250: b'\\xfa', 251: b'\\xfb', 252: b'\\xfc', 253: b'\\xfd', 254: b'\\xfe', 255: b'\\xff', 256: b'<|endoftext|>'}\n",
      "\n",
      "\n",
      "==================================================\n",
      "BPE Tokenizer merges list (post-training)\n",
      "==================================================\n",
      "[(79, 110), (110, 99), (99, 101), (32, 117), (117, 112), (112, 111), (111, 110), (32, 116), (116, 105), (105, 109), (109, 101), (32, 116), (116, 104), (104, 101), (101, 114), (114, 101), (32, 119), (119, 97), (97, 115), (32, 108), (108, 105), (105, 116), (116, 116), (116, 108), (108, 101), (32, 98), (98, 111), (111, 121), (32, 110), (110, 97)]\n",
      "\n",
      "\n",
      "==================================================\n",
      "BPE Tokenizer vocab cache (post-training)\n",
      "==================================================\n",
      "[(-41764510, (46,)), (-23284330, (44,)), (-20828576, (32, 116)), (-15018967, (10,)), (-15063529, (32, 97)), (-20828576, (116, 104)), (-20828576, (104, 101)), (-4840028, (32, 72)), (-10593232, (32, 119)), (-10593232, (119, 97)), (-14903559, (32, 116)), (-5138607, (105, 116)), (-5226508, (32, 84)), (-4774529, (32, 34)), (-19475966, (32, 97)), (-3848306, (32, 105)), (-4623365, (32, 84)), (-4840028, (72, 101)), (-3781381, (32, 104)), (-3781381, (104, 105)), (-2509817, (32, 118)), (-2434613, (39, 115)), (-14903559, (116, 111)), (-2717701, (124, 62)), (-4370380, (32, 115)), (-5226508, (84, 104)), (-2335215, (108, 101)), (-2466207, (32, 116)), (-2171910, (32, 102)), (-2550422, (111, 110)), (-19475966, (97, 110)), (-3848306, (105, 110)), (-3840758, (32, 104)), (-4216755, (32, 100)), (-4623365, (84, 104)), (-2948466, (32, 116)), (-3691718, (32, 84)), (-2096034, (101, 114)), (-2875873, (32, 104)), (-3781381, (105, 115)), (-1297407, (32, 104)), (-1159705, (32, 100)), (-2509817, (101, 114)), (-1257028, (32, 100)), (-2335215, (32, 108)), (-10593232, (97, 115)), (-5138607, (32, 105)), (-2158385, (105, 109)), (-1883362, (33,)), (-4370380, (115, 97))]\n"
     ]
    }
   ],
   "source": [
    "l_50 = 50*\"=\"\n",
    "print(l_50)\n",
    "print(\"TRAINING TIME (seconds):\")\n",
    "print(l_50)\n",
    "print(total_time)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "l_50 = 50*\"=\"\n",
    "print(l_50)\n",
    "print(\"BPE Tokenizer vocab (post-training)\")\n",
    "print(l_50)\n",
    "\n",
    "print(bpe.vocab)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(l_50)\n",
    "print(\"BPE Tokenizer merges list (post-training)\")\n",
    "print(l_50)\n",
    "print(bpe.merges[:30])\n",
    "\n",
    "print(\"\\n\")\n",
    "print(l_50)\n",
    "print(\"BPE Tokenizer vocab cache (post-training)\")\n",
    "print(l_50)\n",
    "\n",
    "print(bpe._vocab_cache[:50])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
