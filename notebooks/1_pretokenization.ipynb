{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEMO 1: **Pre-Tokenization**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo run of pre-tokenization over text with a Macbook Pro 2023, M3 Pro. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-tokenization Pattern\n",
    "\n",
    "Regex-based pattern (used by GPT-2; Radford et al., 2019) from github.com/openai/tiktoken/pull/234/files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-tokenization pattern\n",
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will process the TinyStories validation .txt file, downloaded from HuggingFace, for faster demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Input file to pretokenize: TinyStories validation .txt file, from Hugging Face.\n",
    "INPUT_PATH = Path(\"../data/TinyStoriesV2-GPT4-valid.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## •• Run Pretokenization ••\n",
    "\n",
    "Both parallel and serial runtimes are displayed for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the number of available CPU cores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import cpu_count\n",
    "\n",
    "n_cpus = cpu_count()\n",
    "print(n_cpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run pre-tokenization utilising all the cpus available (`n_cpus`) with a Macbook Pro 2023, M3 Pro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bpe_transformer.tokenization.preprocessing import pretokenize\n",
    "\n",
    "# End of Text token, to split the chunks.\n",
    "split_token = b\"<|endoftext|>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Parallelized Version ===\n",
      "Processed 5501965 tokens in 0.56 seconds\n",
      "Found 13113 unique tokens\n",
      "Most common tokens: [((46,), 421616), ((44,), 235432), ((32, 116, 104, 101), 211031), ((32, 97, 110, 100), 196057), ((32, 97), 152161), ((10,), 152067), ((32, 116, 111), 150493), ((32, 119, 97, 115), 108019), ((32, 84, 104, 101, 121), 52425), ((32, 105, 116), 51670)]\n",
      "\n",
      "=== Serial Version ===\n",
      "Processed 5501965 tokens in 3.07 seconds\n",
      "Found 13113 unique tokens\n",
      "Most common tokens: [((46,), 421616), ((44,), 235432), ((32, 116, 104, 101), 211031), ((32, 97, 110, 100), 196057), ((32, 97), 152161), ((10,), 152067), ((32, 116, 111), 150493), ((32, 119, 97, 115), 108019), ((32, 84, 104, 101, 121), 52425), ((32, 105, 116), 51670)]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "pretokens_counter = pretokenize(file_path=INPUT_PATH, split_token=split_token, n_workers=n_cpus)\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"\\n=== Parallelized Version ===\")\n",
    "print(f\"Processed {sum(pretokens_counter.values())} tokens in {end_time - start_time:.2f} seconds\")\n",
    "print(f\"Found {len(pretokens_counter)} unique tokens\")\n",
    "print(f\"Most common tokens: {pretokens_counter.most_common(10)}\")\n",
    "\n",
    "\n",
    "# Serialized run\n",
    "start_time = time.time()\n",
    "pretokens_counter = pretokenize(file_path=INPUT_PATH, split_token=split_token, parallel_processing=False)\n",
    "end_time = time.time()\n",
    "print(\"\\n=== Serial Version ===\")\n",
    "print(f\"Processed {sum(pretokens_counter.values())} tokens in {end_time - start_time:.2f} seconds\")\n",
    "print(f\"Found {len(pretokens_counter)} unique tokens\")\n",
    "print(f\"Most common tokens: {pretokens_counter.most_common(10)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show that, generally, pretokenizing the chunks in parallel w/ all cpus (total `cpu_count()`) takes aprox. **20\\%** of the total time of a serialized run.\n",
    "\n",
    "It should be worth implementing this processing in a language like C++ or Rust."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try pre-tokenizing on **TinyStories train.txt** :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input file to pretokenize: TinyStories validation .txt file, from Hugging Face.\n",
    "INPUT_PATH = Path(\"../data/TinyStoriesV2-GPT4-train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Parallelized Version ===\n",
      "Processed 544752481 tokens in 41.59 seconds\n",
      "Found 59921 unique tokens\n",
      "Most common tokens: [((46,), 41764510), ((44,), 23284330), ((32, 116, 104, 101), 20828576), ((32, 97, 110, 100), 19475966), ((32, 97), 15063529), ((10,), 15018967), ((32, 116, 111), 14903559), ((32, 119, 97, 115), 10593232), ((32, 84, 104, 101, 121), 5226508), ((32, 105, 116), 5138607)]\n",
      "\n",
      "=== Serial Version ===\n",
      "Processed 544752481 tokens in 307.37 seconds\n",
      "Found 59921 unique tokens\n",
      "Most common tokens: [((46,), 41764510), ((44,), 23284330), ((32, 116, 104, 101), 20828576), ((32, 97, 110, 100), 19475966), ((32, 97), 15063529), ((10,), 15018967), ((32, 116, 111), 14903559), ((32, 119, 97, 115), 10593232), ((32, 84, 104, 101, 121), 5226508), ((32, 105, 116), 5138607)]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "pretokens_counter = pretokenize(file_path=INPUT_PATH, split_token=split_token, n_workers=n_cpus)\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"\\n=== Parallelized Version ===\")\n",
    "print(f\"Processed {sum(pretokens_counter.values())} tokens in {end_time - start_time:.2f} seconds\")\n",
    "print(f\"Found {len(pretokens_counter)} unique tokens\")\n",
    "print(f\"Most common tokens: {pretokens_counter.most_common(10)}\")\n",
    "\n",
    "\n",
    "# Serialized run\n",
    "start_time = time.time()\n",
    "pretokens_counter = pretokenize(file_path=INPUT_PATH, split_token=split_token, parallel_processing=False)\n",
    "end_time = time.time()\n",
    "print(\"\\n=== Serial Version ===\")\n",
    "print(f\"Processed {sum(pretokens_counter.values())} tokens in {end_time - start_time:.2f} seconds\")\n",
    "print(f\"Found {len(pretokens_counter)} unique tokens\")\n",
    "print(f\"Most common tokens: {pretokens_counter.most_common(10)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gap gets more noticeable as the dataset gets larger -- for the TinyStories train dataset, the parallelized run takes **aprox. 15%** of the total time needed for serialized version."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
